{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa2ab696-7e96-426c-a8d6-93cc1d1b5c8f",
   "metadata": {},
   "source": [
    "**Device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ffd282-73b3-4693-bd90-0877fde4faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Returns the best available device (CUDA, MPS, or CPU).\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The best available device ('cuda', 'mps', or 'cpu').\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17418694",
   "metadata": {},
   "source": [
    "# Face Generation\n",
    "\n",
    "We will define and train a **Style Generative Adverserial Network** on a dataset of faces. The goal is to get a generator network to generate *new* images of faces that look as realistic as possible!\n",
    "\n",
    "### Get the Data\n",
    "\n",
    "We'll be using the [CelebFaces Attributes Dataset (CelebA)](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) to train your adversarial networks.\n",
    "\n",
    "This dataset has higher resolution images than datasets like MNIST or SVHN and so, we should prepare to define deeper networks, StleGAN generator, and train them for a longer time to get good results. It is suggested that we utilize a GPU for training.\n",
    "\n",
    "### Pre-processed Data\n",
    "\n",
    "Since we are focused is on building the GANs, we will be using a pre-processed data by [Udacity](https://udacity.com/). Each of the CelebA images has been cropped to remove parts of the image that don't include a face, then resized down to 64x64x3 NumPy images. Some sample data is show below.\n",
    "\n",
    "<img src='../assets/processed_face_data.png' width=60% />\n",
    "\n",
    "> Please download this data from Udacity [by clicking here](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5be7eb6f_processed-celeba-small/processed-celeba-small.zip)\n",
    "\n",
    "This is a zip file and after extracting in the home directory of this notebook for further loading and processing. After extracting the data, we should be left with a directory of data `processed-celeba-small/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c8788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from typing import Tuple, Callable, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms as T\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb834b",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "The [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset contains over 200,000 celebrity images with annotations. Since we're going to be generating faces, we won't need the annotations, we'll only need the images. Note that these are color images with [3 color channels (RGB)](https://en.wikipedia.org/wiki/Channel_(digital_image)#RGB_Images) each.\n",
    "\n",
    "### Pre-process and Load the Data\n",
    "\n",
    "* Implement the input data transformation\n",
    "* Create a custom Dataset class that reads the CelebA data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c82bb0",
   "metadata": {},
   "source": [
    "### `get_transforms` function\n",
    "\n",
    "The `get_transforms` function outputs a [`torchvision.transforms.Compose`](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose) of different transformations. We have two constraints:\n",
    "* the function takes a tuple of size as input and should **resize the images** to the input size\n",
    "* the output images have values **ranging from -1 to 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(size: Tuple[int, int]) -> Callable:\n",
    "    \"\"\"Transforms to apply to the image.\"\"\"\n",
    "    transforms = [\n",
    "        T.Resize(size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]\n",
    "        ),  # Scale values to range [-1, 1]\n",
    "    ]\n",
    "\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e525b70f",
   "metadata": {},
   "source": [
    "### Implement the DatasetDirectory class\n",
    "\n",
    "The `DatasetDirectory` class is a torch Dataset that reads from the above data directory. The `__getitem__` method outpust a transformed tensor and the `__len__` method outputs the number of files in our dataset. Look at [this custom dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) for ideas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf57756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "class DatasetDirectory(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class that loads images from a specified folder.\n",
    "\n",
    "    Args:\n",
    "    - directory: Location of the images.\n",
    "    - transforms: Transform function to apply to the images.\n",
    "    - extension: File format of the images (default is '.jpg').\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, directory: str, transforms: Callable = None, extension: str = \".jpg\"\n",
    "    ):\n",
    "        self.directory = directory\n",
    "        self.transforms = transforms\n",
    "        self.extension = extension\n",
    "        # Get a list of all files in the directory with the specified extension\n",
    "        self.image_paths = [\n",
    "            os.path.join(directory, file)\n",
    "            for file in os.listdir(directory)\n",
    "            if file.endswith(self.extension)\n",
    "        ]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of items in the dataset.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        \"\"\"Loads an image and applies transformations.\"\"\"\n",
    "        # Load the image\n",
    "        img_path = self.image_paths[index]\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Ensure 3 channels\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae7865-8276-4413-8f4c-5b49a4083032",
   "metadata": {},
   "source": [
    "Initialize `image_size` by 64 and `img_channels` by 3 because we will generate 64 by 64 RGB images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85778214-da3b-40e7-8ddb-e0b7d4d9e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "img_channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69773ebc-aca8-4366-9b58-47c3a2fffa46",
   "metadata": {},
   "source": [
    "Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15514d7e-4f99-44ac-adac-8248839efc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Define the filename, URL, and folder name\n",
    "filename = \"processed-celeba-small.zip\"\n",
    "url = \"https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5be7eb6f_processed-celeba-small/processed-celeba-small.zip\"\n",
    "folder_name = \"processed_celeba_small\"\n",
    "\n",
    "\n",
    "# Download progress callback\n",
    "def download_progress_hook(t):\n",
    "    \"\"\"Download progress bar hook.\"\"\"\n",
    "    last_b = [0]\n",
    "\n",
    "    def update_to(b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            t.total = tsize\n",
    "        t.update((b - last_b[0]) * bsize)\n",
    "        last_b[0] = b\n",
    "\n",
    "    return update_to\n",
    "\n",
    "\n",
    "# Check if the folder already exists\n",
    "if not os.path.exists(folder_name):\n",
    "    # Download if the folder doesn't exist\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Downloading file...\")\n",
    "        with tqdm(unit=\"B\", unit_scale=True, unit_divisor=1024) as t:\n",
    "            urllib.request.urlretrieve(\n",
    "                url, filename, reporthook=download_progress_hook(t)\n",
    "            )\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(\"File already downloaded. Skipping download.\")\n",
    "\n",
    "    # Unzip the file with progress\n",
    "    print(\"Unzipping file...\")\n",
    "    with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "        for file in tqdm(\n",
    "            iterable=zip_ref.namelist(),\n",
    "            total=len(zip_ref.namelist()),\n",
    "            desc=\"Extracting\",\n",
    "        ):\n",
    "            zip_ref.extract(file)\n",
    "    print(\"Unzipping complete.\")\n",
    "else:\n",
    "    print(\"Data already exists. Skipping download and unzip.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f838b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"processed_celeba_small/celeba/\"\n",
    "\n",
    "# run this cell to create the dataset\n",
    "dataset = DatasetDirectory(data_dir, get_transforms((image_size, image_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e43c8e",
   "metadata": {},
   "source": [
    "Let's visualize images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(images):\n",
    "    \"\"\"Transform images from [-1.0, 1.0] to [0, 255] and cast them to uint8.\"\"\"\n",
    "    return ((images + 1.0) / 2.0 * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "plot_size = 20\n",
    "for idx in np.arange(plot_size):\n",
    "    ax = fig.add_subplot(2, int(plot_size / 2), idx + 1, xticks=[], yticks=[])\n",
    "    img = dataset[idx].numpy()\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img = denormalize(img)\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88387901",
   "metadata": {},
   "source": [
    "## Model implementation\n",
    "\n",
    "As we already know, a GAN is comprised of two adversarial networks, a discriminator and a generator. Now that we have a working data pipeline, we need to implement the discriminator and the generator. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f82c35",
   "metadata": {},
   "source": [
    "### Create the discriminator\n",
    "\n",
    "The discriminator's job is to score real and fake images. We have two constraints here:\n",
    "* the discriminator takes as input a **batch of 64x64x3 images**\n",
    "* the output should be a single value (=score)\n",
    "\n",
    "Feel free to get inspiration from the different architectures, such as DCGAN, WGAN-GP or DRAGAN.\n",
    "\n",
    "#### Some tips\n",
    "* To scale down from the input image, one can either use `Conv2d` layers with the correct hyperparameters or Pooling layers.\n",
    "* If one is planning on using gradient penalty, then we should not use Batch Normalization layers in the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa60bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15111efa-6435-472b-8c5f-c9d0bf14574c",
   "metadata": {},
   "source": [
    "The class `WSConv2d` (weighted scaled convolutional layer) to Equalized Learning Rate for the conv layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa39eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (2 / (in_channels * (kernel_size**2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9127995-f8e7-4ca8-89d8-3e320f52ce59",
   "metadata": {},
   "source": [
    "The class Discriminator is the same as in [ProGAN paper](https://arxiv.org/pdf/1710.10196)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fce06b-bd51-47ac-8176-012469b29d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # here we work back ways from factors because the discriminator\n",
    "        # should be mirrored from the generator. So the first prog_block and\n",
    "        # rgb layer we append will work for input size 1024x1024, then 512->256-> etc\n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            conv_in = int(in_channels * factors[i])\n",
    "            conv_out = int(in_channels * factors[i - 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "        # perhaps confusing name \"initial_rgb\" this is just the RGB layer for 4x4 input size\n",
    "        # did this to \"mirror\" the generator initial_rgb\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(\n",
    "            kernel_size=2, stride=2\n",
    "        )  # down sampling using avg pool\n",
    "\n",
    "        # this is the block for 4x4 input size\n",
    "        self.final_block = nn.Sequential(\n",
    "            # +1 to in_channels because we concatenate from MiniBatch std\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(\n",
    "                in_channels, 1, kernel_size=1, padding=0, stride=1\n",
    "            ),  # we use this instead of linear layer\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        \"\"\"Used to fade in downscaled using avg pooling and output from CNN\"\"\"\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        batch_statistics = (\n",
    "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        )\n",
    "        # we take the std for each example (across all channels, and pixels) then we repeat it\n",
    "        # for a single channel and concatenate it with the image. In this way the discriminator\n",
    "        # will get information about the variation in the batch/image\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        # where we should start in the list of prog_blocks, maybe a bit confusing but\n",
    "        # the last is for the 4x4. So example let's say steps=1, then we should start\n",
    "        # at the second to last because input_size will be 8x8. If steps==0 we just\n",
    "        # use the final block\n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "\n",
    "        # convert from rgb as initial step, this will depend on\n",
    "        # the image size (each will have it's on rgb layer)\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0:  # i.e, image is 4x4\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        # because prog_blocks might change the channels, for down scale we use rgb_layer\n",
    "        # from previous/smaller size which in our case correlates to +1 in the indexing\n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "\n",
    "        # the fade_in is done first between the downscaled and the input\n",
    "        # this is opposite from the generator\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d12e0f-7427-4970-8661-4b528f6dae79",
   "metadata": {},
   "source": [
    "In the original paper, they initialize z_dim, w_dim, and inchannels by 512. But considering the simple pre-processed data  andto use less VRAM usage and speed-up training, we will be using 128. We could perhaps even get better results if we doubled them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0865ded8-0ebf-4df9-b338-0e47650e33c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 128\n",
    "z_dim = 128\n",
    "w_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451fcaf-e7d6-4867-85f3-647a6cfbced0",
   "metadata": {},
   "source": [
    "Let's define a variable with the name `factors` that contain the numbers that will multiply with in_channels to have the number of channels that we want in each image resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fdc56f-869c-45e1-afbe-c9c62e02cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78be5abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to create the Discriminator (critic)\n",
    "critic = Discriminator(in_channels, img_channels=img_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf367119-5ce6-4c86-9ad4-a6291447e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd12da8",
   "metadata": {},
   "source": [
    "### Create the generator\n",
    "\n",
    "The generator's job creates the \"fake images\" and learns the dataset distribution. We have three constraints here:\n",
    "* the generator takes as input a vector of dimension `[batch_size, latent_dimension, 1, 1]`\n",
    "* the generator must outputs **64x64x3 images**\n",
    "\n",
    "Feel free to get inspiration from the different architectures, such as DCGAN, WGAN-GP or DRAGAN.\n",
    "\n",
    "#### Some tips:\n",
    "* to scale up from the latent vector input, you can use `ConvTranspose2d` layers\n",
    "* as often with Gan, **Batch Normalization** helps with training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a037eb7-bbd2-4f57-8139-cc0f47dce2d1",
   "metadata": {},
   "source": [
    "#### The WSLinear (Weighted Scaled Linear) Class\n",
    "\n",
    "- In the initialization, we pass in `in_features` and `out_channels`. A linear layer is created, followed by defining a scale factor, which is set as the square root of 2 divided by `in_features`. The bias from the current linear layer is stored separately to prevent it from being affected by scaling, after which it is removed. Finally, the linear layer is initialized.\n",
    "- In the forward method, we pass `x` and simply multiply it by the scale factor, then add the bias after reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190dbf45-c104-42d2-ad36-0ebcedb9e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.scale = (2 / in_features) ** 0.5\n",
    "        self.bias = self.linear.bias\n",
    "        self.linear.bias = None\n",
    "\n",
    "        # initialize linear layer\n",
    "        nn.init.normal_(self.linear.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x * self.scale) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ef0b4-65ea-4fdd-bf15-2e63c5a79305",
   "metadata": {},
   "source": [
    "#### Adaptive Instance Normalization (AdaIN) Class\n",
    "\n",
    "- In the initialization method, we pass in `channels` and `w_dim`. We set up an instance normalization layer for normalizing the input and create `style_scale` and `style_bias`, which serve as the adaptive components. These are implemented using `WSLinear`, which maps the Noise Mapping Network (W) to match the `channels`.\n",
    "\n",
    "- In the forward pass, we pass `x`, apply instance normalization to it, and then return the result of `style_scale * x + style_bias`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab8e57-e6bc-4f11-9cb8-08a9d3c9e2bc",
   "metadata": {},
   "source": [
    "The PixelNorm class to normalize Z before the Noise Mapping Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54410d3-502b-41a2-8a24-3e6403de4646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0454ef5e-0f2d-4733-a517-7aff13fb9a71",
   "metadata": {},
   "source": [
    "#### MappingNetwork Class\n",
    "\n",
    "In the initialization method, we pass in `z_dim` and `w_dim`. The network consists of:\n",
    "- A normalization layer for `z_dim`.\n",
    "- A sequence of eight fully connected layers using `WSLinear`, each followed by a ReLU activation function, mapping `z_dim` to `w_dim`.\n",
    "\n",
    "In the forward method, we apply this mapping to the input and return the transformed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce77c1-4dbb-4b6b-baa0-1fa640a7357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim):\n",
    "        super().__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            WSLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mapping(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd95459a-9f12-43b0-b605-65e545e584c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, channels, w_dim):\n",
    "        super().__init__()\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
    "        self.style_scale = WSLinear(w_dim, channels)\n",
    "        self.style_bias = WSLinear(w_dim, channels)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        x = self.instance_norm(x)\n",
    "        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)\n",
    "        style_bias = self.style_bias(w).unsqueeze(2).unsqueeze(3)\n",
    "        return style_scale * x + style_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c561c-5697-42ff-b54e-992d364b99f8",
   "metadata": {},
   "source": [
    "#### InjectNoise Class\n",
    "\n",
    "- In the initialization method, we pass in `channels`. We initialize `weight` using a random normal distribution and use `nn.Parameter` so that this weight becomes trainable and can be optimized during training.\n",
    "- In the forward method, we pass an image `x` and return it with added random noise, allowing dynamic variations in the generator's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac84671-e860-49c0-83d0-afa9547b009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InjectNoise(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise = torch.randn((x.shape[0], 1, x.shape[2], x.shape[3]), device=x.device)\n",
    "        return x + self.weight * noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7711e0-558e-422d-912e-f8b2e733756e",
   "metadata": {},
   "source": [
    "#### GenBlock Class\n",
    "\n",
    "In the initialization method, we pass `in_channels`, `out_channels`, and `w_dim`. The following components are then set up:\n",
    "- `conv1`: A convolutional layer using `WSConv2d` that maps `in_channels` to `out_channels`.\n",
    "- `conv2`: Another `WSConv2d` layer that maps `out_channels` to `out_channels`.\n",
    "- `leaky`: A Leaky ReLU activation with a slope of 0.2, as specified in the paper.\n",
    "- `inject_noise1` and `inject_noise2`: Instances of `InjectNoise` to add noise at different stages.\n",
    "- `adain1` and `adain2`: Instances of `AdaIN` to normalize and modulate the features adaptively.\n",
    "\n",
    "In the forward method, we pass `x` through the following sequence:\n",
    "1. Apply `conv1`, followed by `inject_noise1`, then apply `leaky`, and normalize with `adain1`.\n",
    "2. Pass the result through `conv2`, followed by `inject_noise2`, apply `leaky`, and normalize with `adain2`.\n",
    "\n",
    "Finally, we return the modified `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64175004-d32b-4701-a457-80d71d4c4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, w_dim):\n",
    "        super(GenBlock, self).__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.inject_noise1 = InjectNoise(out_channels)\n",
    "        self.inject_noise2 = InjectNoise(out_channels)\n",
    "        self.adain1 = AdaIN(out_channels, w_dim)\n",
    "        self.adain2 = AdaIN(out_channels, w_dim)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        x = self.adain1(self.leaky(self.inject_noise1(self.conv1(x))), w)\n",
    "        x = self.adain2(self.leaky(self.inject_noise2(self.conv2(x))), w)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77637838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim, in_channels, img_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.starting_constant = nn.Parameter(torch.ones((1, in_channels, 4, 4)))\n",
    "        self.map = MappingNetwork(z_dim, w_dim)\n",
    "        self.initial_adain1 = AdaIN(in_channels, w_dim)\n",
    "        self.initial_adain2 = AdaIN(in_channels, w_dim)\n",
    "        self.initial_noise1 = InjectNoise(in_channels)\n",
    "        self.initial_noise2 = InjectNoise(in_channels)\n",
    "        self.initial_conv = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.prog_blocks, self.rgb_layers = (\n",
    "            nn.ModuleList([]),\n",
    "            nn.ModuleList([self.initial_rgb]),\n",
    "        )\n",
    "\n",
    "        for i in range(\n",
    "            len(factors) - 1\n",
    "        ):  # -1 to prevent index error because of factors[i+1]\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i + 1])\n",
    "            self.prog_blocks.append(GenBlock(conv_in_c, conv_out_c, w_dim))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "\n",
    "    def forward(self, noise, alpha, steps):\n",
    "        w = self.map(noise)\n",
    "        x = self.initial_adain1(self.initial_noise1(self.starting_constant), w)\n",
    "        x = self.initial_conv(x)\n",
    "        out = self.initial_adain2(self.leaky(self.initial_noise2(x)), w)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(x)\n",
    "\n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"bilinear\")\n",
    "            out = self.prog_blocks[step](upscaled, w)\n",
    "\n",
    "        # The number of channels in upscale will stay the same, while\n",
    "        # out which has moved through prog_blocks might change. To ensure\n",
    "        # we can convert both to rgb we use different rgb_layers\n",
    "        # (steps-1) and steps for upscaled, out respectively\n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to create the generator\n",
    "generator = Generator(z_dim, w_dim, in_channels, img_channels=img_channels).to(device)\n",
    "generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a14d9d",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "In the following section, we create the optimizers for the generator and discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1947761a",
   "metadata": {},
   "source": [
    "### Implement the optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd65bffb-daaa-4ace-b28c-e1366d5b6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lr = 0.001\n",
    "d_lr = 0.001\n",
    "g_betas = (0.0, 0.99)\n",
    "d_betas = (0.0, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def create_optimizers(generator: nn.Module, discriminator: nn.Module):\n",
    "    \"\"\"This function should return the optimizers of the generator and the discriminator\"\"\"\n",
    "    # TODO: implement the generator and discriminator optimizers\n",
    "    g_optimizer = optim.Adam(\n",
    "        [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    param\n",
    "                    for name, param in generator.named_parameters()\n",
    "                    if \"map\" not in name\n",
    "                ]\n",
    "            },\n",
    "            {\"params\": generator.map.parameters(), \"lr\": 1e-5},\n",
    "        ],\n",
    "        lr=g_lr,\n",
    "        betas=g_betas,\n",
    "    )\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=d_lr, betas=d_betas)\n",
    "    return g_optimizer, d_optimizer\n",
    "\n",
    "\n",
    "g_optimizer, d_optimizer = create_optimizers(generator, critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac695b4b",
   "metadata": {},
   "source": [
    "## Losses implementation\n",
    "\n",
    "In this section, we will implement the loss function for the generator and the discriminator. \n",
    "\n",
    "Some tips:\n",
    "* Choose the commonly used the binary cross entropy loss or select other losses, such as the Wasserstein distance.\n",
    "* Implement a gradient penalty function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f8c1a",
   "metadata": {},
   "source": [
    "### Implement the generator loss\n",
    "\n",
    "The generator's goal is to get the discriminator to think its generated images (= \"fake\" images) are real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe57e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_logits):\n",
    "    \"\"\"\n",
    "    Computes the loss for the generator in a GAN using the Wasserstein loss formulation.\n",
    "\n",
    "    Parameters:\n",
    "    - fake_logits (Tensor): Logits from the discriminator for generated (fake) images.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: Calculated generator loss.\n",
    "    \"\"\"\n",
    "    # The generator aims to maximize the discriminator's response on fake images.\n",
    "    # In Wasserstein GANs, this corresponds to maximizing the mean of fake logits.\n",
    "    # Using negative sign to achieve a minimization objective for gradient descent.\n",
    "    return -torch.mean(fake_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59193e",
   "metadata": {},
   "source": [
    "### Implement the discriminator loss\n",
    "\n",
    "We want the discriminator to give high scores to real images and low scores to fake ones and the discriminator loss should reflect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits, gp, lambda_gp=10, drift_penalty=0.001):\n",
    "    \"\"\"\n",
    "    Computes the loss for the discriminator in a GAN, incorporating real and fake logits,\n",
    "    gradient penalty, and a drift penalty.\n",
    "\n",
    "    Parameters:\n",
    "    - real_logits (Tensor): Logits from the discriminator for real images.\n",
    "    - fake_logits (Tensor): Logits from the discriminator for generated (fake) images.\n",
    "    - gp (Tensor): Gradient penalty term to enforce the Lipschitz constraint.\n",
    "    - lambda_gp (float): Weight for the gradient penalty term. Default is 10.\n",
    "    - drift_penalty (float): Weight for the drift penalty term to stabilize training. Default is 0.001.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: Calculated discriminator loss.\n",
    "    \"\"\"\n",
    "    # Wasserstein loss for real and fake logits\n",
    "    wasserstein_loss = -(torch.mean(real_logits) - torch.mean(fake_logits))\n",
    "\n",
    "    # Gradient penalty weighted by lambda_gp\n",
    "    gradient_penalty_term = lambda_gp * gp\n",
    "\n",
    "    # Drift penalty to regularize the discriminator on real logits (helps prevent exploding gradients)\n",
    "    drift_penalty_term = drift_penalty * torch.mean(real_logits**2)\n",
    "\n",
    "    # Total discriminator loss\n",
    "    return wasserstein_loss + gradient_penalty_term + drift_penalty_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88f2ab",
   "metadata": {},
   "source": [
    "### Implement the gradient Penalty\n",
    "\n",
    "We know the importance of gradient penalty in training certain types of Gans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cc0d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(\n",
    "    discriminator, real_samples, fake_samples, alpha, train_step, device=\"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the gradient penalty for Wasserstein GAN with gradient penalty (WGAN-GP).\n",
    "\n",
    "    Parameters:\n",
    "    - discriminator (nn.Module): The discriminator model.\n",
    "    - real_samples (Tensor): Batch of real images.\n",
    "    - fake_samples (Tensor): Batch of fake images generated by the generator.\n",
    "    - alpha (float): Mixing factor for progressive growing (usually between 0 and 1).\n",
    "    - train_step (int): Current training step, used for updating the discriminator progressively.\n",
    "    - device (str): Device to perform calculations on (e.g., 'cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: Calculated gradient penalty.\n",
    "    \"\"\"\n",
    "    batch_size, channels, height, width = real_samples.shape\n",
    "\n",
    "    # Random weight for interpolation between real and fake samples\n",
    "    beta = torch.rand((batch_size, 1, 1, 1), device=device).expand_as(real_samples)\n",
    "    interpolated_images = real_samples * beta + fake_samples.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    # Compute discriminator scores on interpolated images\n",
    "    mixed_scores = discriminator(interpolated_images, alpha, train_step)\n",
    "\n",
    "    # Calculate gradients of scores with respect to interpolated images\n",
    "    gradient = torch.autograd.grad(\n",
    "        outputs=mixed_scores,\n",
    "        inputs=interpolated_images,\n",
    "        grad_outputs=torch.ones_like(mixed_scores, device=device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    # Flatten gradients per example and compute L2 norm\n",
    "    gradient = gradient.view(batch_size, -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "\n",
    "    # Gradient penalty term enforcing the gradient norm to be close to 1\n",
    "    gp = torch.mean((gradient_norm - 1) ** 2)\n",
    "\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d15b5d",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "\n",
    "Training will involve alternating between training the discriminator and the generator.\n",
    "\n",
    "* We will train the discriminator by alternating on real and fake images\n",
    "* Then the generator, which tries to trick the discriminator and therefore have an opposing loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113f1dd",
   "metadata": {},
   "source": [
    "### Implement the generator step and the discriminator step functions\n",
    "\n",
    "Each function do the following:\n",
    "* calculate the loss\n",
    "* backpropagate the gradient\n",
    "* perform one optimizer step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def generator_step(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    batch_size: int,\n",
    "    latent_dim: int,\n",
    "    alpha: float,\n",
    "    step: int,\n",
    "    gen_optimizer,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    One training step of the generator.\n",
    "\n",
    "    Parameters:\n",
    "    - generator (nn.Module): The generator model.\n",
    "    - discriminator (nn.Module): The discriminator model.\n",
    "    - batch_size (int): Current batch size.\n",
    "    - latent_dim (int): Dimension of the latent space.\n",
    "    - alpha (float): Mixing factor for progressive growing (usually between 0 and 1).\n",
    "    - step (int): Current training step, used for progressive growing.\n",
    "    - gen_optimizer (Optimizer): Optimizer for the generator.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, float]: Dictionary containing the generator loss.\n",
    "    \"\"\"\n",
    "    # Sample random noise as input for the generator\n",
    "    noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "\n",
    "    # Generate fake images\n",
    "    fake_images = generator(noise, alpha, step)\n",
    "\n",
    "    # Compute the generator loss\n",
    "    fake_logits = discriminator(fake_images, alpha, step)\n",
    "    g_loss = generator_loss(fake_logits)\n",
    "\n",
    "    # Backpropagation and optimization step\n",
    "    gen_optimizer.zero_grad()\n",
    "    g_loss.backward()\n",
    "    gen_optimizer.step()\n",
    "\n",
    "    return {\"loss\": g_loss.item()}\n",
    "\n",
    "\n",
    "def discriminator_step(\n",
    "    discriminator,\n",
    "    generator,\n",
    "    real_images: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    latent_dim: int,\n",
    "    alpha: float,\n",
    "    step: int,\n",
    "    lambda_gp: float,\n",
    "    disc_optimizer,\n",
    "    drift_penalty=0.001,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    One training step of the discriminator.\n",
    "\n",
    "    Parameters:\n",
    "    - discriminator (nn.Module): The discriminator model.\n",
    "    - generator (nn.Module): The generator model.\n",
    "    - real_images (Tensor): Batch of real images.\n",
    "    - batch_size (int): Current batch size.\n",
    "    - latent_dim (int): Dimension of the latent space.\n",
    "    - alpha (float): Mixing factor for progressive growing (usually between 0 and 1).\n",
    "    - step (int): Current training step, used for progressive growing.\n",
    "    - lambda_gp (float): Weight for the gradient penalty.\n",
    "    - disc_optimizer (Optimizer): Optimizer for the discriminator.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, float]: Dictionary containing the discriminator loss and gradient penalty.\n",
    "    \"\"\"\n",
    "    # Move real images to device\n",
    "    real_images = real_images.to(device)\n",
    "\n",
    "    # Generate fake images with the generator\n",
    "    noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "    fake_images = generator(noise, alpha, step).detach()\n",
    "\n",
    "    # Discriminator logits for real and fake images\n",
    "    real_logits = discriminator(real_images, alpha, step)\n",
    "    fake_logits = discriminator(fake_images, alpha, step)\n",
    "\n",
    "    # Compute gradient penalty\n",
    "    gp = gradient_penalty(\n",
    "        discriminator, real_images, fake_images, alpha, step, device=device\n",
    "    )\n",
    "\n",
    "    # Discriminator loss: Wasserstein loss with gradient penalty and drift penalty\n",
    "    d_loss = discriminator_loss(real_logits, fake_logits, gp, lambda_gp, drift_penalty)\n",
    "\n",
    "    # Backpropagation and optimization step\n",
    "    disc_optimizer.zero_grad()\n",
    "    d_loss.backward()\n",
    "    disc_optimizer.step()\n",
    "\n",
    "    return {\"loss\": d_loss.item(), \"gp\": gp.item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90eeb6",
   "metadata": {},
   "source": [
    "### Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb25ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab225234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of images in each batch\n",
    "batch_sizes = [64, 32, 16, 8]\n",
    "\n",
    "progressive_epochs = [30] * len(batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887aecc-bec5-47d8-a717-13d531af08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(image_size):\n",
    "    transform = get_transforms((image_size, image_size))\n",
    "    batch_size = batch_sizes[int(log2(image_size / 4))]\n",
    "    dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=False\n",
    "    )\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c01910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(fixed_latent_vector: torch.Tensor):\n",
    "    \"\"\"helper function to display images during training\"\"\"\n",
    "    fig = plt.figure(figsize=(14, 4))\n",
    "    plot_size = 16\n",
    "    for idx in np.arange(plot_size):\n",
    "        ax = fig.add_subplot(2, int(plot_size / 2), idx + 1, xticks=[], yticks=[])\n",
    "        img = fixed_latent_vector[idx, ...].detach().cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = denormalize(img)\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe99b30",
   "metadata": {},
   "source": [
    "### Implement the training strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069c150-0003-4172-8e26-2e07e3a60e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train_at_img_size = 8\n",
    "lambda_gp = 10\n",
    "alpha = 1e-5  # start with very low alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129fd8e8-04f1-496d-a596-3e467c2c5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "def generate_examples(gen, steps, n=100):\n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(1, z_dim).to(device)\n",
    "            img = gen(noise, alpha, steps)\n",
    "            if not os.path.exists(f\"saved_examples/step{steps}\"):\n",
    "                os.makedirs(f\"saved_examples/step{steps}\")\n",
    "            save_image(img * 0.5 + 0.5, f\"saved_examples/step{steps}/img_{i}.png\")\n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac006d-b478-46fd-98a7-fc752380a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start at the step that corresponds to the initial image size in config\n",
    "step = int(log2(start_train_at_img_size / 4))\n",
    "losses = []  # To store losses for plotting later\n",
    "fixed_latent_vector = (\n",
    "    torch.randn(16, z_dim).float().to(device)\n",
    ")  # Fixed vector for consistent image generation\n",
    "print_every = 200  # Frequency of printing loss updates\n",
    "\n",
    "for num_epochs in progressive_epochs[step:]:\n",
    "    alpha = 1e-5  # Start with a very low alpha\n",
    "    loader, dataset = get_loader(4 * 2**step)\n",
    "    print(f\"Current image size: {4 * 2 ** step}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training loop for each batch\n",
    "        for batch_idx, (real_images, _) in enumerate(loader):\n",
    "            cur_batch_size = real_images.shape[0]\n",
    "\n",
    "            # Discriminator Step\n",
    "            d_loss = discriminator_step(\n",
    "                discriminator=critic,\n",
    "                generator=generator,\n",
    "                real_images=real_images,\n",
    "                batch_size=cur_batch_size,\n",
    "                latent_dim=z_dim,\n",
    "                alpha=alpha,\n",
    "                step=step,\n",
    "                lambda_gp=lambda_gp,\n",
    "                disc_optimizer=d_optimizer,\n",
    "            )\n",
    "\n",
    "            # Generator Step\n",
    "            g_loss = generator_step(\n",
    "                generator=generator,\n",
    "                discriminator=critic,\n",
    "                batch_size=cur_batch_size,\n",
    "                latent_dim=z_dim,\n",
    "                alpha=alpha,\n",
    "                step=step,\n",
    "                gen_optimizer=g_optimizer,\n",
    "            )\n",
    "\n",
    "            # Update alpha based on the current batch size\n",
    "            alpha += cur_batch_size / ((progressive_epochs[step] * 0.5) * len(dataset))\n",
    "            alpha = min(alpha, 1)  # Ensure alpha does not exceed 1\n",
    "\n",
    "            # Store losses and print them periodically\n",
    "            if batch_idx % print_every == 0:\n",
    "                losses.append(\n",
    "                    (d_loss[\"loss\"], g_loss[\"loss\"])\n",
    "                )  # Append both losses for later plotting\n",
    "                time = str(datetime.now()).split(\".\")[0]\n",
    "                print(\n",
    "                    f\"{time} | Epoch [{epoch+1}/{num_epochs}] | Batch {batch_idx}/{len(loader)} | \"\n",
    "                    f\"d_loss: {d_loss['loss']:.4f} | g_loss: {g_loss['loss']:.4f} | gp: {d_loss['gp']:.4f}\"\n",
    "                )\n",
    "\n",
    "        # Generate and display images at the end of each epoch\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            generated_images = generator(fixed_latent_vector, alpha, step)\n",
    "            display(generated_images)\n",
    "        generator.train()\n",
    "\n",
    "    step += 1  # Progress to the next image size for the generator and discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779bee7",
   "metadata": {},
   "source": [
    "### Training losses\n",
    "\n",
    "Plot the training losses for the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb18d57-8eca-4248-87d0-c466f572a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label=\"Discriminator\", alpha=0.5)\n",
    "plt.plot(losses.T[1], label=\"Generator\", alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
